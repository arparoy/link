name: Scrape Blog Links

on:
  workflow_dispatch:  # Manual trigger
  schedule:
    - cron: '0 0 * * 1'  # Runs every Monday at 00:00 UTC

jobs:
  scrape:
    runs-on: ubuntu-latest

    steps:
      - name: â¬‡ï¸ Checkout Repository
        uses: actions/checkout@v3

      - name: ðŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.x'

      - name: ðŸ“¦ Install dependencies
        run: pip install beautifulsoup4 requests

      - name: ðŸš€ Run the scraper
        run: python scrape_links.py

      - name: ðŸ§ª Check for changes
        id: check_changes
        run: |
          git config --global user.name 'github-actions'
          git config --global user.email 'github-actions@github.com'
          git add output.txt
          if git diff --cached --quiet; then
            echo "changed=false" >> $GITHUB_OUTPUT
          else
            echo "changed=true" >> $GITHUB_OUTPUT
          fi

      - name: âœ… Commit updated output.txt
        if: steps.check_changes.outputs.changed == 'true'
        run: |
          git commit -m "Update output.txt with new links"
          git push

      - name: ðŸ—‚ Upload output.txt as artifact
        uses: actions/upload-artifact@v4
        with:
          name: blog-links
          path: output.txt
